# -*- coding: utf-8 -*-
"""CHATBOT PROJE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XAjMSdSYPGDNS3e_RoS3cnY1eZof7dV3
"""

import re
import random 


class RuleBot:

  # potential negative responses
  negative_responses = ("no","nope","nah","naw","hayır","not a chance","sorry","üzgünüm")
  
  # potential positive responses
  positive_responses = ("yes","elbette","tabi ki","evet","neden olmasın.")
  # keywords for exiting the conversation
  exit_commands = ("çıkış yapmak","Duraklat","dur","çıkış","exit","goodbye","güle güle","bye","later","hoşçakal")

  # random starter questions
  random_questions = (
      "Neden buradasın? ",
      "Senin gibi çok insan var mı? ",
      "Yaşamak için ne tüketirsiniz?",
      "Bu gezegende Akıllı yaşam var mı? ",
      "Dünyanın Lideri var mı? ",
      "Hangi Ülkeleri ziyaret ettin? ",
      "Bu gezegende hangi teknolojiye sahipsiniz?"

  )

  def  __init__(self):
    self.alienbabble = {"describe_planet_intent": r'.*\s*your planet.*',
                            "answer_why_intent": r'why\sare.*',
                            "about_intellipat": r'.*\s*intellipaat'}


  def greet(self):

     self.name = input("What is your Name?\n")
     will_help = input(
         f"Hi {self.name},Ben Altunar-Bot.Gezegeninizi öğrenmeme yardım eder misiniz?\n")
     if will_help in self.negative_responses:
       print("Tamam, güzel bir Dünya Günü geçirin!")
       return
     self.chat()

     if will_help in self.positive_responses:
       print("Ohh.Yeni bilgiler öğrenmeyi sabırsızlıkla bekliyorum.:)")
       return 
     self.chat() 


  def make_exit(self,reply):
    for command in self.exit_commands:
      if reply == command:
        print("Tamam, güzel bir Dünya Günü geçirin!")
        return True


  def chat(self):
    reply = input(random.choice(self.random_questions)).lower()
    while not self.make_exit(reply):
      reply = input(self.match_reply(reply))

  def match_reply(self,reply):
    for key,value in self.alienbabble.items():
      intent = key
      regex_pattern = value
      found_match = re.match(regex_pattern,reply)
      if found_match and intent == "describe_planet_intent":
        return self.describe_planet_intent()

      elif found_match and intent ==  "answer_why_intent":
        return self.answer_why_intent()
      
      elif found_match and intent == "about_intellipat":
        return self.about_intellipat()

      if not found_match:
        return self.no_match_intent()

  
  def describe_planet_intent(self):
    responses = ("Gezegenim, çeşitli organizmaların ve boşlukların bir ütopyasıdır.\n",
                "Ben Asi Galaksilerin başkenti İstanbul'danım.\n")
    return random.choice(responses)

  def answer_why_intent(self):
    responses = ("huzur içinde geldim\n", "Gezegeniniz ve sakinleri hakkında veri toplamak için buradayım\n",
                 "Türk Kadınlarının Egolu olduğunu duydum\n"
)
    return random.choice(responses)

  def about_intellipat(self):
    responses = ("Ali Fidan'a Göre,Mahmud Sami Ramazanoğlu İmam Hatip lisesi, dünyanın en güzel kızlarını barındırır.\n",

              "Ali Altunar Gaydir.\n",
              "Ali Altunara Gay Demeyen Gaydir.\n")
    return random.choice(responses)

  def no_match_intent(self):
    responses = (
        "Lütfen bana daha fazlasını anlat.\n",
        "Bana daha fazlasını anlat!\n",

        "Bunu neden söylüyorsun?\n",

        "Anlıyorum. Detaylandırabilir misin?\n",

        "Anlıyorum. Detaylandırabilir misin?\n",
        "Anlıyorum. Sizce nasıl?\n",

        "Neden?\n",

        "Bunu söylediğinde nasıl hissettiğimi sanıyorsun?\n"
    )
    return random.choice(responses)

TalkingBot = RuleBot()
TalkingBot.greet()

"""CREATING NLTK CHATBOT İN PYTHON

PYTHON'DA NLTK CHATBOT OLUŞTURMA


"""

import numpy as np
import nltk
import string
import random

f = open("data.txt","r",errors = "ignore")

raw_doc = f.read()

raw_doc

raw_doc = raw_doc.lower() # Converting entire text to lowercase  # Metnin tamamını küçük harfe dönüştürme

nltk.download("punkt")
# Using the Punkt tokenizer 
# Punkt belirteci kullanma
# NLTK'nin (' Punkt ') kullanımı nedir? simgeleştir. punkt modülü. Bu belirteç oluşturucu, 
# kısaltma sözcükleri, eşdizimler ve cümleleri başlatan sözcükler için bir model oluşturmak üzere denetimsiz bir algoritma kullanarak bir metni cümleler listesine böler.

nltk.download("wordnet") # Using the wordnet dictionary # Wordnet sözlüğünü kullanma

nltk.download("omw-1.4") 
sentence_tokens = nltk.sent_tokenize(raw_doc)
word_tokens = nltk.word_tokenize(raw_doc)

raw_doc

"""AFTER TOKENIZATION

TOKENİZASYONDAN SONRA

"""

sentence_tokens[:4]
# ilk dört cümleyi yazdırır.

word_tokens[:5]
# ilk 4 kelimeyi yazdırır

"""PERFORMING TEXT-PROCESSING STEPS

METİN İŞLEME ADIMLARINI GERÇEKLEŞTİRME

"""

lemmer = nltk.stem.WordNetLemmatizer()

def lemTokens(tokens):
  return[lemmer.lemmatize(token) for token in tokens]

remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)

# ord() işlevi, belirtilen bir karakterin unicode kodunu temsil eden sayıyı döndürür.
# Önceki bölümü özetlemek gerekirse: bir Unicode dizesi, 0 ile 0x10FFFF (1.114.111 ondalık) 
# arasındaki sayılar olan bir kod noktaları dizisidir. Bu kod noktaları dizisinin bellekte 
# bir dizi kod birimi olarak temsil edilmesi gerekir ve kod birimleri daha sonra 8 bitlik baytlara eşlenir.


# string.punctuation --> Dize noktalama işaretleri, Python3'ün dize modülünde önceden tanımlanmıştır. 
# Tüm karakterleri bir dizi olarak içerir. Programın her yerinde kullanabiliriz.11 Temmuz 2020


def LemNormalize(text):
  return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))

"""DEFINE GREETING FUNCTIONS

"""

greet_inputs = ("Hello","Hi","What's Up","How are you?","Merhaba")
greet_responses = ("Hi","Hey","Hey There!","There There!!")

def greet(sentence):
  for word in sentence.split():
    if word.lower() in greet_inputs:
      return random.choice(greet_responses)

"""RESPONSE GENERATION BY THE BOT

BOT TARAFINDAN YANIT ÜRETİMİ

"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity # iki vektörün benzerliğini ölçmeye yarar.

def response(user_response):
  robo1_response = " "
  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize,stop_words = "english")
  tfidf = TfidfVec.fit_transform(sentence_tokens)
  vals = cosine_similarity(tfidf[-1],tfidf)
  idx = vals.argsort()[0][-2]
  flat = vals.flatten()
  flat.sort()
  req_tfidf = flat[-2]
  if (req_tfidf == 0):
    robo1_response = robo1_response + " I am Sorry. Unable to understand you!"
    return robo1_response

  else:
    robo1_response = robo1_response + sentence_tokens[idx]
    return robo1_response

"""DEFINING THE CHATFLOW

SOHBET AKIŞINI TANIMLAMAK


"""

flag = True
print("Hello! I am the Retreival Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye!")
while(flag == True):
  user_response = input()
  user_response = user_response.lower()
  
  if(user_response != "bye"):
    if(user_response == "thank you" or user_response == "thanks"):
      flag = False
      print("Bot: You are Welcome..")
    else:
      if(greet(user_response) != None):
        print("Bot" + greet(user_response))

      else:
        sentence_tokens.append(user_response)
        word_tokens = word_tokens + nltk.word_tokenize(user_response)
        final_words = list(set(word_tokens))
        print("Bot: ", end = " ")
        print(response(user_response))
        sentence_tokens.remove(user_response) # Remove() yöntemi, belirtilen değere sahip öğenin ilk geçtiği yeri kaldırır.

  else:
    flag = False
    print("Bot: GoodBye! " )

