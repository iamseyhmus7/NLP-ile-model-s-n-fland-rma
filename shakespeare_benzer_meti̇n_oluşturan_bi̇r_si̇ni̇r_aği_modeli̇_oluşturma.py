# -*- coding: utf-8 -*-
"""SHAKESPEARE BENZER METİN OLUŞTURAN BİR SİNİR AĞI MODELİ OLUŞTURMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cfZiG-oQsG35Gk8AQfhcUfF2OjTXTrLc
"""

with open("shakespeare.txt","r",encoding = "utf-8") as f:
  text = f.read()

text

len(text)

print(text[:1000])

# here are all the unique characters that occur in this text
# işte bu metinde geçen tüm benzersiz karakterler

chars = sorted(list(set(text)))
vocab_size = len(chars)
print("".join(chars))
print(vocab_size)

# create a mapping from characters to integers
# karakterlerden tam sayılara bir eşleme oluştur

stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers   # kodlayıcı: bir dizi alın, tam sayıların bir listesini çıkarın

decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string   # kod çözücü: tamsayıların bir listesini alın, bir dize yazdırın


print(encode("hii there"))
print(decode(encode("hii there")))

import torch

data = torch.tensor(encode(text),dtype = torch.long)
print(data.shape,data.dtype)
print(data[:1000])

# Let's now split up the data info train and validation sets
# Şimdi veri bilgi dizisi ve doğrulama kümelerini ayıralım
#Train and test splits

n = int(0.9*len(data)) # first 90% will be train,rest val  # ilk %90 tren, dinlenme val olacak
train_data = data[:n]
val_data = data[n:]

block_size = 8
train_data[:block_size +1]

X = train_data[:block_size]
y = train_data[:block_size + 1]

for t in range(block_size):
  context = X[:t+1]
  target = y[t]
  print(f"When input is {context} the target:{target}")

torch.manual_seed(1337)
batch_size = 4 # how many independent sequences will we process in parallel?
block_size = 8 # what is maximum context length for predictions?


# data loading
# veriler yükle
def get_batch(split):
  # generate a small batch of data of inputs x and targets y 

  data = train_data if split == "train" else val_data
  ix = torch.randint(len(data) - block_size,(batch_size,))
  x = torch.stack([data[i:i+block_size] for i in ix])
  y = torch.stack([data[i+1:i+block_size + 1] for i in ix])
  return x,y


xb,yb = get_batch("train")
print("inputs:")
print(xb.shape)
print(xb)
print("targets:")
print(yb.shape)
print(yb)

print("----")

for b in range(batch_size): # batch dimension
  for t in range(block_size): # time dimension
      context = xb[b,:t+1]
      target = yb[b,t]
      print(f"When input is {context.tolist()} the target:{target}")

print(xb)

import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

# super simple bigram model
# süper basit bigram modeli


class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        # her belirteç, bir sonraki belirteç için günlükleri doğrudan bir arama tablosundan okur

        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):

        # idx and targets are both (B,T) tensor of integers
        # idx ve hedeflerin her ikisi de (B,T) tam sayıların tensörüdür

        logits = self.token_embedding_table(idx) # (B,T,C)
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss
    
    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        # idx, geçerli bağlamdaki (B, T) dizin dizisidir

        for _ in range(max_new_tokens):
            # get the predictions
            # tahminleri al

            logits, loss = self(idx)
            # focus only on the last time step
            # sadece son adıma odaklan

            logits = logits[:, -1, :] # becomes (B, C) # (B, C) olur

            # apply softmax to get probabilities
            # olasılıkları elde etmek için softmax uygulayın

            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distributio
            # dağıtımdan örnek

            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            # çalışan diziye örneklenmiş dizin ekleyin

            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
print(logits.shape)
print(loss)

print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))

# create a pyTorch optimizer 
 # bir pyTorch iyileştirici oluştur

 optimizer = torch.optim.AdamW(m.parameters(),lr = 1e-3)

batch_size = 32

# increase number of steps for good results...
# iyi sonuçlar için adım sayısını artırın...

for steps in range(10000):

    # sample a batch of data
    # bir grup veriyi örnekleyin

    xb,yb = get_batch("train")

    # evaluate the loss
    # kaybı değerlendir
 
    logits,loss = m(xb,yb)
    optimizer.zero_grad(set_to_none = True)
    loss.backward()
    optimizer.step()


print(loss.item())

# generate from the model 
# modelden üret

print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))

